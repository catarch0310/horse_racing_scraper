import pandas as pd
from datetime import datetime
import os
import importlib
import google.generativeai as genai
import time
from difflib import SequenceMatcher
import re

# --- 1. AI æ ¸å¿ƒè¨­å®š ---
API_KEY = os.getenv("GEMINI_API_KEY")

def init_ai():
    if not API_KEY:
        print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GEMINI_API_KEY")
        return None
    try:
        genai.configure(api_key=API_KEY)
        # ç›´æ¥ä½¿ç”¨æ¨¡å‹åç¨±ï¼Œé¿é–‹ models/ å‰ç¶´ä»¥ç›¸å®¹ v1beta
        return genai.GenerativeModel('gemini-1.5-flash')
    except Exception as e:
        print(f"âŒ AI åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

model = init_ai()

# --- 2. æ™ºæ…§ç¿»è­¯åŠŸèƒ½ (åªç¿»è­¯ä¸­æ—¥æ–‡ï¼Œç¯€çœ 70% Token) ---
def is_need_translation(text):
    """æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ—¥æ–‡å­—å…ƒ"""
    # åŒ¹é…ä¸­æ–‡å­—å…ƒ (\u4e00-\u9fff) æˆ– æ—¥æ–‡å‡å (\u3040-\u30ff)
    if re.search(r'[\u4e00-\u9fff\u3040-\u30ff]', text):
        return True
    return False

def translate_titles_smartly(all_data):
    if not model or not all_data:
        return all_data
    
    # 1. ç¯©é¸å‡ºéœ€è¦ç¿»è­¯çš„é …ç›® (è¨˜éŒ„ç´¢å¼•)
    to_translate_indices = []
    titles_to_send = []
    
    for idx, item in enumerate(all_data):
        if is_need_translation(item['title']):
            to_translate_indices.append(idx)
            titles_to_send.append(item['title'])
    
    if not titles_to_send:
        print("âœ… æ‰€æœ‰æ¨™é¡Œå‡ç‚ºè‹±æ–‡ï¼Œè·³éç¿»è­¯æ­¥é©Ÿã€‚")
        return all_data

    print(f"ğŸŒ ç™¼ç¾ {len(titles_to_send)} å‰‡ä¸­/æ—¥æ–‡æ¨™é¡Œï¼Œæº–å‚™ç¿»è­¯ (å…¶é¤˜ {len(all_data)-len(titles_to_send)} å‰‡è·³é)...")
    
    # 2. åˆ†æ®µç¿»è­¯ (æ¯ 50 å‰‡ä¸€çµ„)
    chunk_size = 50
    for i in range(0, len(titles_to_send), chunk_size):
        chunk_titles = titles_to_send[i : i + chunk_size]
        chunk_indices = to_translate_indices[i : i + chunk_size]
        
        prompt = (
            "Translate these Japanese or Chinese horse racing headlines into English. "
            "Return ONLY the translations, one per line, strictly maintaining the order:\n\n" 
            + "\n".join(chunk_titles)
        )
        
        try:
            response = model.generate_content(prompt)
            translated_lines = response.text.strip().split('\n')
            
            # å°‡ç¿»è­¯çµæœå¡å›åŸå§‹æ•¸æ“š
            for j, orig_idx in enumerate(chunk_indices):
                if j < len(translated_lines):
                    en_text = translated_lines[j].strip()
                    # é¿å… AI å»¢è©±æˆ–é‡è¤‡
                    if en_text and len(en_text) > 5:
                        all_data[orig_idx]['title'] = f"{all_data[orig_idx]['title']} ({en_text})"
            
            print(f"   âœ… å·²å®Œæˆç¬¬ {i+1} è‡³ {min(i + chunk_size, len(titles_to_send))} å‰‡ç¿»è­¯")
            time.sleep(3) # é¿é–‹é »ç‡é™åˆ¶
        except Exception as e:
            print(f"   âš ï¸ æ­¤æ®µç¿»è­¯å¤±æ•—: {str(e)[:50]}")
            
    return all_data

# --- 3. æˆ°ç•¥åˆ†æå ±å‘Š (è‹±æ–‡) ---
def generate_strategic_report(all_headlines):
    if not model: return "AI Model Not Ready."

    # æ ¼å¼åŒ–æ¸…å–®ä¾› AI åˆ†æ (æœ€å¤šåˆ†æ 200 å‰‡ç²¾è¯)
    news_text = ""
    for i, item in enumerate(all_headlines[:200]):
        news_text += f"ID: {i+1} | Source: {item['source']} | Title: {item['title']}\n"

    prompt = f"""
    # Role
    You are a Strategic Industry Analyst for global horse racing. 
    Review these headlines and provide a brief in ENGLISH for senior editors.

    # Raw Data
    {news_text}

    # Task
    1. **TOP 5 STRATEGIC KEYWORDS**: List 5 most important themes and briefly explain why they are trending.
    2. **OUTLIER RADAR**: Identify 2-3 niche/unusual stories with potential global impact.

    Format with professional Markdown.
    """

    try:
        safety = [{"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}]
        response = model.generate_content(prompt, safety_settings=safety)
        return response.text.strip()
    except Exception as e:
        return f"Report failed: {str(e)}"

# --- 4. è³‡æ–™æ¸…æ´— ---
def similarity(a, b):
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def deduplicate_data(all_data):
    if not all_data: return []
    unique_url_data = []
    seen_urls = set()
    for item in all_data:
        if item['link'] not in seen_urls:
            seen_urls.add(item['link'])
            unique_url_data.append(item)
    
    final_data = []
    for item in unique_url_data:
        is_duplicate = False
        for existing_item in final_data:
            if similarity(item['title'], existing_item['title']) > 0.85:
                is_duplicate = True
                break
        if not is_duplicate:
            final_data.append(item)
    return final_data

# --- 5. ç¸½åŸ·è¡Œæµç¨‹ ---
def run_all():
    all_data = []
    SITES = ['racing_post', 'scmp_racing', 'singtao_racing', 'punters_au', 'racing_com', 'tospo_keiba', 'netkeiba_news', 'bloodhorse_news', 'the_straight', 'anz_bloodstock', 'ttr_ausnz', 'smh_racing', 'drf_news', 'racenet_news', 'daily_telegraph', 'equidia_racing']
    
    for site in SITES:
        try:
            print(f"\n>>> åŸ·è¡Œ: {site}")
            module = importlib.import_module(f"scrapers.{site}")
            data = module.scrape()
            if data:
                for item in data: item['source'] = site
                all_data.extend(data)
                print(f"    âœ… æŠ“åˆ° {len(data)} å‰‡")
        except Exception as e:
            print(f"    âŒ {site} éŒ¯èª¤: {e}")

    if all_data:
        # A. æ•¸æ“šå»é‡
        all_data = deduplicate_data(all_data)
        
        # B. æ™ºæ…§ç¿»è­¯ (åªç¿»è­¯éè‹±æ–‡æ¨™é¡Œ)
        all_data = translate_titles_smartly(all_data)

        date_str = datetime.now().strftime('%Y%m%d')
        os.makedirs('data', exist_ok=True)

        # C. å„²å­˜åŸå§‹ CSV
        df = pd.DataFrame(all_data)
        df.to_csv(f"data/raw_news_{date_str}.csv", index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ CSV å·²å­˜è‡³: data/raw_news_{date_str}.csv")

        # D. ç”Ÿæˆå ±å‘Š
        print(f"\nğŸ¤– ç”Ÿæˆæˆ°ç•¥å ±å‘Š...")
        report = generate_strategic_report(all_data)
        with open(f"data/strategic_report_{date_str}.md", "w", encoding="utf-8") as f:
            f.write(report)
        print(f"âœ¨ æˆ°ç•¥å ±å‘Šå®Œæˆ: strategic_report_{date_str}.md")
    else:
        print("\nâŒ ä»Šæ—¥ç„¡æ–°èã€‚")

if __name__ == "__main__":
    run_all()
