import pandas as pd
from datetime import datetime
import os
import importlib
import google.generativeai as genai
import time
from difflib import SequenceMatcher

# --- 1. AI æ ¸å¿ƒè¨­å®š ---
# é€™è£¡æœƒå¾ GitHub Secrets è®€å– Key
API_KEY = os.getenv("GEMINI_API_KEY")

def init_ai():
    """ 
    æœ€ç°¡åŒ–åˆå§‹åŒ–ï¼šåªè¦æœ‰ Key å°±å¼·è¡Œå»ºç«‹æ¨¡å‹ç‰©ä»¶
    """
    if not API_KEY:
        print("âŒ éŒ¯èª¤ï¼šGEMINI_API_KEY è®Šæ•¸ç‚ºç©ºï¼Œè«‹æª¢æŸ¥ GitHub Secretsã€‚")
        return None
    
    print(f"ğŸ”‘ åµæ¸¬åˆ° API KEYï¼Œæ­£åœ¨é…ç½®æ¨¡å‹...")
    try:
        genai.configure(api_key=API_KEY)
        # ä½¿ç”¨æœ€ç©©å®šçš„æ¨¡å‹è·¯å¾‘
        return genai.GenerativeModel('gemini-1.5-flash')
    except Exception as e:
        print(f"âŒ AI é…ç½®å‡ºéŒ¯: {e}")
        return None

# åˆå§‹åŒ–
model_instance = init_ai()

# --- 2. æ¨™é¡Œç¿»è­¯ (å¯«å…¥æ¨™é¡Œå¾Œæ–¹) ---
def translate_titles_to_en(all_data):
    if not model_instance or not all_data:
        return all_data
    
    print(f"ğŸŒ æ­£åœ¨ç¿»è­¯ {len(all_data)} å‰‡æ¨™é¡Œ...")
    raw_titles = [item['title'] for item in all_data]
    # ä¸€æ¬¡è™•ç†æ‰€æœ‰æ¨™é¡Œ
    titles_blob = "\n".join(raw_titles)
    
    prompt = f"Translate the following racing headlines into English. Return ONLY the English text, one per line, strictly maintaining the order. If a line is already English, keep it as is:\n\n{titles_blob}"
    
    try:
        response = model_instance.generate_content(prompt)
        translated_lines = response.text.strip().split('\n')
        
        # æ•¸é‡å°é½Šæ‰è™•ç†ï¼Œé˜²æ­¢è³‡æ–™éŒ¯ä½
        if len(translated_lines) >= len(all_data):
            for i in range(len(all_data)):
                orig = all_data[i]['title']
                en = translated_lines[i].strip()
                # ç°¡å–®åˆ¤æ–·ï¼šå¦‚æœç¿»è­¯å…§å®¹è·ŸåŸæ–‡ä¸ä¸€æ¨£ï¼ˆä»£è¡¨åŸæ–‡æ˜¯ä¸­/æ—¥æ–‡ï¼‰ï¼Œæ‰é™„åŠ 
                if orig.lower() != en.lower():
                    all_data[i]['title'] = f"{orig} ({en})"
            print("âœ… æ¨™é¡Œè‹±è­¯é™„åŠ æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ ç¿»è­¯å¤±æ•—: {e}")
    
    return all_data

# --- 3. æˆ°ç•¥åˆ†æå ±å‘Š (è‹±æ–‡) ---
def generate_strategic_report(all_headlines):
    if not model_instance:
        return "AI Model Not Ready. Please check GEMINI_API_KEY."

    news_text = ""
    for i, item in enumerate(all_headlines):
        news_text += f"ID: {i+1} | Source: {item['source']} | Title: {item['title']}\n"

    prompt = f"""
    # Role
    You are a Strategic Industry Analyst for global horse racing.
    Analyze the following headlines and provide a brief in ENGLISH:

    # Input Data
    {news_text}

    # Task
    1. **TOP 5 STRATEGIC KEYWORDS**: List 5 most important themes from the data and briefly explain why they matter.
    2. **OUTLIER RADAR**: Identify 2-3 "unusual" or "niche" headlines that represent unique industry shifts or local incidents worth deeper investigation.

    Format with Markdown. Be concise and professional.
    """

    try:
        # å¼·åˆ¶é—œé–‰å®‰å…¨æ€§éæ¿¾ï¼Œé˜²æ­¢è³½é¦¬è©å½™è¢«æ“‹
        safety = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        response = model_instance.generate_content(
            prompt, 
            generation_config={"temperature": 0.2},
            safety_settings=safety
        )
        return response.text.strip()
    except Exception as e:
        return f"Report failed: {str(e)}"

# --- 4. è³‡æ–™æ¸…æ´— ---
def similarity(a, b):
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def deduplicate_data(all_data):
    if not all_data: return []
    unique_url_data = []
    seen_urls = set()
    for item in all_data:
        if item['link'] not in seen_urls:
            seen_urls.add(item['link'])
            unique_url_data.append(item)
    
    final_data = []
    for item in unique_url_data:
        is_duplicate = False
        for existing_item in final_data:
            if similarity(item['title'], existing_item['title']) > 0.85:
                is_duplicate = True
                break
        if not is_duplicate:
            final_data.append(item)
    return final_data

# --- 5. åŸ·è¡Œæµç¨‹ ---
def run_all():
    all_data = []
    SITES = ['racing_post', 'scmp_racing', 'singtao_racing', 'punters_au', 'racing_com', 'tospo_keiba', 'netkeiba_news', 'bloodhorse_news', 'the_straight', 'anz_bloodstock', 'ttr_ausnz', 'smh_racing', 'drf_news', 'racenet_news', 'daily_telegraph', 'equidia_racing']
    
    for site in SITES:
        try:
            print(f">>> Task: {site}")
            module = importlib.import_module(f"scrapers.{site}")
            data = module.scrape()
            if data:
                for item in data: item['source'] = site
                all_data.extend(data)
        except Exception as e:
            print(f"âŒ {site} Error: {e}")

    if all_data:
        # å»é‡
        all_data = deduplicate_data(all_data)
        
        # ç¿»è­¯
        all_data = translate_titles_to_en(all_data)

        date_str = datetime.now().strftime('%Y%m%d')
        os.makedirs('data', exist_ok=True)

        # å„²å­˜ CSV
        df = pd.DataFrame(all_data)
        df.to_csv(f"data/raw_news_{date_str}.csv", index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ CSV Saved.")

        # ç”¢å‡ºå ±å‘Š
        print(f"ğŸ¤– Generating Strategic Report...")
        ai_report = generate_strategic_report(all_data)
        with open(f"data/strategic_report_{date_str}.md", "w", encoding="utf-8") as f:
            f.write(ai_report)
        print(f"âœ¨ Report Saved.")
    else:
        print("âŒ No data.")

if __name__ == "__main__":
    run_all()
