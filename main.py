import pandas as pd
from datetime import datetime
import os
import importlib
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
import time

# --- AI è¨­å®š ---
API_KEY = os.getenv("GEMINI_API_KEY")

def init_ai_model():
    if not API_KEY:
        print("âŒ æ‰¾ä¸åˆ° API KEY")
        return None
    try:
        genai.configure(api_key=API_KEY)
        # å˜—è©¦ç©©å®šç‰ˆåç¨±ï¼Œé¿é–‹ v1beta 404 å ±éŒ¯
        # å»ºè­°ä½¿ç”¨ gemini-1.5-flash æˆ– gemini-2.0-flash-exp (æœ€æ–°)
        for model_name in ['gemini-1.5-flash', 'gemini-1.5-flash-latest', 'gemini-2.0-flash']:
            try:
                m = genai.GenerativeModel(model_name)
                # æ¸¬è©¦ä¸€ä¸‹æ¨¡å‹æ˜¯å¦å¯ç”¨
                m.generate_content("test", generation_config={"max_output_tokens": 1})
                print(f"âœ… AI æˆåŠŸå•Ÿç”¨æ¨¡å‹: {model_name}")
                return m
            except:
                continue
        return None
    except Exception as e:
        print(f"AI åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

model = init_ai_model()

def generate_summary_with_retry(title, content, retries=2):
    if not model: return "AI æ¨¡å‹æœªå°±ç·’"
    
    text_to_analyze = content[:2000] if len(content) > 50 else f"æ ¹æ“šæ¨™é¡Œåˆ†æï¼š{title}"
    prompt = f"ä½ æ˜¯è³½é¦¬å°ˆå®¶ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å°‡ä»¥ä¸‹å…§å®¹ç¸½çµæˆä¸€å¥50å­—å…§çš„ç²¾é—¢æ‘˜è¦ï¼š\n\n{text_to_analyze}"

    for i in range(retries):
        try:
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            if "429" in str(e):
                print(f"      â³ è§¸ç™¼é »ç‡é™åˆ¶ï¼Œä¼‘æ¯ 10 ç§’å¾Œé‡è©¦...")
                time.sleep(10)
            else:
                return f"æ‘˜è¦å¤±æ•—: {str(e)[:50]}"
    return "æ‘˜è¦å˜—è©¦å¤šæ¬¡å¤±æ•—"

def get_full_text(url, source):
    if source == 'on_cc_racing': return ""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')
        # æŠ“å– <p> æ¨™ç±¤å…§æ–‡
        paragraphs = soup.find_all('p')
        return "\n".join([p.get_text() for p in paragraphs if len(p.get_text()) > 30])[:2500]
    except:
        return ""

def run_all():
    all_data = []
    SITES = ['racing_post', 'scmp_racing', 'on_cc_racing', 'punters_au']
    
    for site in SITES:
        try:
            print(f"\n>>> çˆ¬å– {site}...")
            module = importlib.import_module(f"scrapers.{site}")
            data = module.scrape()
            if data:
                for item in data: item['source'] = site
                all_data.extend(data)
                print(f"    âœ… {site} æˆåŠŸæŠ“åˆ° {len(data)} å‰‡")
        except Exception as e:
            print(f"    âŒ {site} éŒ¯èª¤: {e}")

    if all_data:
        print(f"\nğŸ¤– é€²è¡Œ AI æ‘˜è¦ (å…± {len(all_data)} å‰‡)...")
        results = []
        for i, item in enumerate(all_data):
            print(f"    ({i+1}/{len(all_data)}) è™•ç†: {item['title'][:20]}")
            content = get_full_text(item['link'], item['source'])
            item['ai_summary'] = generate_summary_with_retry(item['title'], content)
            results.append(item)
            # é…åˆå…è²»ç‰ˆé™åˆ¶ï¼Œæ¯å‰‡ä¼‘æ¯ 5 ç§’
            time.sleep(5)

        df = pd.DataFrame(results)
        os.makedirs('data', exist_ok=True)
        filename = f"data/ai_racing_report_{datetime.now().strftime('%Y%m%d')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… å ±å‘Šå­˜æª”: {filename}")

if __name__ == "__main__":
    run_all()
