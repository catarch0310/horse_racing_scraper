import pandas as pd
from datetime import datetime
import os
import importlib
import google.generativeai as genai
import time
from difflib import SequenceMatcher

# --- AI è¨­å®šèˆ‡è‡ªå‹•åµæ¸¬ (å®Œå…¨ä¿ç•™ä½ çš„ç©©å®šé‚è¼¯) ---
API_KEY = os.getenv("GEMINI_API_KEY")

def get_best_model():
    if not API_KEY: return None
    genai.configure(api_key=API_KEY)
    candidate_names = [
        'gemini-1.5-flash', 
        'models/gemini-1.5-flash', 
        'gemini-1.5-pro',
        'models/gemini-1.5-pro'
    ]
    print("ğŸ¤– æ­£åœ¨åµæ¸¬å¯ç”¨ AI æ¨¡å‹...")
    for name in candidate_names:
        try:
            model = genai.GenerativeModel(name)
            model.generate_content("hi", generation_config={"max_output_tokens": 1})
            print(f"âœ… æˆåŠŸå•Ÿç”¨æ¨¡å‹: {name}")
            return model
        except Exception: continue
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                print(f"âš ï¸ ä½¿ç”¨ç³»çµ±è‡ªå‹•ç™¼ç¾æ¨¡å‹: {m.name}")
                return genai.GenerativeModel(m.name)
    except: pass
    return None

model_instance = get_best_model()

# --- æ ¸å¿ƒä¿®æ­£ï¼šåˆ†æ®µç¿»è­¯åŠŸèƒ½ (é˜²æ­¢ 311 vs 312 éŒ¯èª¤) ---
def translate_titles_to_en(all_data):
    if not model_instance or not all_data: return all_data
    
    print(f"ğŸŒ æ­£åœ¨åˆ†æ®µç¿»è­¯ {len(all_data)} å‰‡æ¨™é¡Œ...")
    
    # æ¯æ¬¡è™•ç† 50 å‰‡ï¼Œé€™æ˜¯ AI æœ€ä¸æœƒæ•¸éŒ¯çš„æ•¸é‡
    chunk_size = 50
    for i in range(0, len(all_data), chunk_size):
        chunk = all_data[i : i + chunk_size]
        raw_titles = [item['title'] for item in chunk]
        
        prompt = (
            "Translate these horse racing headlines into English. "
            "Return ONLY the English translations, one per line, no numbering. "
            "If already in English, keep it as is:\n\n" + "\n".join(raw_titles)
        )

        try:
            response = model_instance.generate_content(prompt)
            translated_lines = response.text.strip().split('\n')
            
            # å¦‚æœé€™ä¸€å°æ®µçš„æ•¸é‡å°ä¸Šäº†ï¼Œå°±é€²è¡Œåˆä½µ
            if len(translated_lines) == len(chunk):
                for j in range(len(chunk)):
                    orig = chunk[j]['title']
                    en = translated_lines[j].strip()
                    if orig.lower() != en.lower():
                        all_data[i + j]['title'] = f"{orig} ({en})"
                print(f"   âœ… å·²å®Œæˆç¬¬ {i+1} è‡³ {min(i + chunk_size, len(all_data))} å‰‡")
            else:
                print(f"   âš ï¸ ç¬¬ {i+1} å€æ®µè¡Œæ•¸ä¸ç¬¦ï¼Œè·³éæ­¤æ®µç¿»è­¯")
            
            # ä¼‘æ¯ 2 ç§’é¿å… API é »ç‡é™åˆ¶
            time.sleep(2)
        except Exception as e:
            print(f"   âš ï¸ ç¬¬ {i+1} å€æ®µç¿»è­¯å‡ºéŒ¯: {e}")
            continue
            
    return all_data

def generate_ai_report(all_headlines):
    if not model_instance: return "AI å ±å‘Šç”Ÿæˆå¤±æ•—ï¼šæ¨¡å‹åˆå§‹åŒ–å¤±æ•—ã€‚"
    news_list_text = ""
    for i, item in enumerate(all_headlines):
        news_list_text += f"{i+1}. [{item['source']}] {item['title']}\n"

    prompt = f"""
    You are a Global Horse Racing Chief Editor. Analyze these headlines:
    {news_list_text}
    Please generate a report in THREE parts: 1. ENGLISH, 2. TRADITIONAL CHINESE (HK), 3. JAPANESE.
    (å…¶é¤˜æŒ‡ä»¤ç¶­æŒåŸæ¨£...)
    """
    try:
        response = model_instance.generate_content(prompt, generation_config={"max_output_tokens": 5000})
        return response.text.strip()
    except Exception as e:
        return f"AI å ±å‘Šå…§å®¹ç”Ÿæˆå‡ºéŒ¯: {str(e)}"

def run_all():
    all_data = []
    SITES = ['racing_post', 'scmp_racing', 'singtao_racing', 'punters_au', 'racing_com', 'tospo_keiba', 'netkeiba_news', 'bloodhorse_news', 'the_straight', 'anz_bloodstock', 'ttr_ausnz', 'smh_racing', 'drf_news', 'racenet_news', 'daily_telegraph', 'equidia_racing']
    
    for site in SITES:
        try:
            print(f"\n>>> ä»»å‹™é–‹å§‹: {site}")
            module = importlib.import_module(f"scrapers.{site}")
            data = module.scrape()
            if data:
                for item in data: item['source'] = site
                all_data.extend(data)
                print(f"    âœ… æŠ“åˆ° {len(data)} å‰‡")
        except Exception as e:
            print(f"    âŒ {site} éŒ¯èª¤: {e}")

    if all_data:
        # ä½¿ç”¨ä¿®æ­£å¾Œçš„åˆ†æ®µç¿»è­¯
        all_data = translate_titles_to_en(all_data)

        date_str = datetime.now().strftime('%Y%m%d')
        os.makedirs('data', exist_ok=True)
        df = pd.DataFrame(all_data)
        df.to_csv(f"data/raw_news_{date_str}.csv", index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ CSV å·²å­˜è‡³: data/raw_news_{date_str}.csv")

        print(f"\nğŸ¤– å•Ÿå‹• AI ç¸½ç·¨è¼¯æ¨¡å¼...")
        ai_report_content = generate_ai_report(all_data)
        with open(f"data/racing_report_{date_str}.md", "w", encoding="utf-8") as f:
            f.write(ai_report_content)
        print(f"âœ¨ AI æˆ°å ±å·²ç”Ÿæˆ: racing_report_{date_str}.md")
    else:
        print("\nâŒ ä»Šæ—¥ç„¡æ–°èæ•¸æ“šã€‚")

if __name__ == "__main__":
    run_all()
