import pandas as pd
from datetime import datetime
import os
import importlib
import google.generativeai as genai
import time
from difflib import SequenceMatcher

# --- 1. AI è¨­å®šèˆ‡è‡ªå‹•åµæ¸¬ (å®Œå…¨ä¿ç•™ä½ çš„ç©©å®šé‚è¼¯) ---
API_KEY = os.getenv("GEMINI_API_KEY")

def get_best_model():
    if not API_KEY: return None
    genai.configure(api_key=API_KEY)
    candidate_names = [
        'gemini-1.5-flash', 
        'models/gemini-1.5-flash', 
        'gemini-1.5-pro',
        'models/gemini-1.5-pro'
    ]
    print("ğŸ¤– æ­£åœ¨åµæ¸¬å¯ç”¨ AI æ¨¡å‹...")
    for name in candidate_names:
        try:
            model = genai.GenerativeModel(name)
            model.generate_content("hi", generation_config={"max_output_tokens": 1})
            print(f"âœ… æˆåŠŸå•Ÿç”¨æ¨¡å‹: {name}")
            return model
        except Exception: continue
    try:
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                print(f"âš ï¸ ä½¿ç”¨ç³»çµ±è‡ªå‹•ç™¼ç¾æ¨¡å‹: {m.name}")
                return genai.GenerativeModel(m.name)
    except: pass
    return None

model_instance = get_best_model()

# --- 2. æ¨™é¡Œç¿»è­¯ (å¢åŠ ä¼‘æ¯æ™‚é–“ä»¥è§£æ±º 429 å•é¡Œ) ---
def translate_titles_to_en(all_data):
    if not model_instance or not all_data: return all_data
    print(f"ğŸŒ æ­£åœ¨åˆ†æ®µç¿»è­¯ {len(all_data)} å‰‡æ¨™é¡Œ...")
    
    # å¢åŠ  chunk_size åˆ° 100 å¯ä»¥æ¸›å°‘è«‹æ±‚æ¬¡æ•¸ï¼Œé™ä½ 429 é¢¨éšª
    chunk_size = 100 
    for i in range(0, len(all_data), chunk_size):
        chunk = all_data[i : i + chunk_size]
        raw_titles = [item['title'] for item in chunk]
        
        prompt = (
            "Translate these horse racing headlines into English. "
            "Return ONLY the English translations, one per line, no numbering. "
            "If already in English, keep it as is:\n\n" + "\n".join(raw_titles)
        )
        try:
            response = model_instance.generate_content(prompt)
            translated_lines = response.text.strip().split('\n')
            
            if len(translated_lines) == len(chunk):
                for j in range(len(chunk)):
                    orig = chunk[j]['title']
                    en = translated_lines[j].strip()
                    if orig.lower() != en.lower():
                        all_data[i + j]['title'] = f"{orig} ({en})"
                print(f"   âœ… å·²å®Œæˆç¬¬ {i+1} è‡³ {min(i + chunk_size, len(all_data))} å‰‡")
            else:
                print(f"   âš ï¸ ç¬¬ {i+1} å€æ®µè¡Œæ•¸ä¸ç¬¦ï¼Œè·³éç¿»è­¯")
            
            # é—œéµä¿®æ”¹ï¼šå»¶é•·ä¼‘æ¯æ™‚é–“è‡³ 6 ç§’ï¼Œç¢ºä¿ä¸è§¸ç™¼ RPM é™åˆ¶
            time.sleep(6) 
        except Exception as e:
            print(f"   âš ï¸ ç¬¬ {i+1} å€æ®µç¿»è­¯å‡ºéŒ¯: {e}")
            time.sleep(10) # å ±éŒ¯å¾Œä¼‘æ¯ä¹…ä¸€é»
            continue
    return all_data

# --- 3. æˆ°ç•¥åˆ†æå ±å‘Š (åŠ å…¥è‡ªå‹•é‡è©¦æ©Ÿåˆ¶) ---
def generate_strategic_brief(all_headlines):
    if not model_instance: return "AI Model Not Ready."

    news_list_text = ""
    for i, item in enumerate(all_headlines):
        news_list_text += f"ID: {i+1} | Source: {item['source']} | Title: {item['title']}\n"

    prompt = f"""
    # Role
    You are a Strategic Industry Analyst for global horse racing. 
    Review the following news headlines collected from global sources (UK, HK, AU, JP, US, FR).

    # Raw Data Input
    {news_list_text}

    # Task (Output in ENGLISH only)
    Perform a cross-check analysis and output the following:

    ## 1. TOP 5 STRATEGIC KEYWORDS
    Identify the 5 most frequent or significant keywords/themes currently trending across global media. 
    For each keyword, briefly explain the industry context.

    ## 2. OUTLIER RADAR (2-3 Items)
    Identify 2-3 specific headlines that are "unusual," "niche," or "out-of-the-ordinary." 
    Explain why a senior editor should look deeper into these.

    # Style
    Authoritative, analytical, and concise. Use professional Markdown headers.
    """

    # é—œéµä¿®æ”¹ï¼šå ±å‘Šç”ŸæˆåŠ å…¥è‡ªå‹•é‡è©¦ (é‡å° 429 éŒ¯èª¤)
    for attempt in range(3):
        try:
            print(f"    ğŸ¤– å˜—è©¦ç”Ÿæˆåˆ†æå ±å‘Š (ç¬¬ {attempt+1} æ¬¡)...")
            safety = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            response = model_instance.generate_content(
                prompt, 
                generation_config={"temperature": 0.2},
                safety_settings=safety
            )
            return response.text.strip()
        except Exception as e:
            if "429" in str(e):
                wait_time = 25 # å¦‚æœè¢«æ“‹ï¼Œå¼·åˆ¶ä¼‘æ¯ 25 ç§’
                print(f"    âš ï¸ è§¸ç™¼é »ç‡é™åˆ¶ï¼Œå¼·åˆ¶ä¼‘æ¯ {wait_time} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
            else:
                return f"Strategic brief generation failed: {str(e)}"
    
    return "Strategic brief generation failed after multiple retries due to rate limits."

# --- 4. è³‡æ–™æ¸…æ´— ---
def similarity(a, b):
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def deduplicate_data(all_data):
    if not all_data: return []
    unique_url_data = []
    seen_urls = set()
    for item in all_data:
        if item['link'] not in seen_urls:
            seen_urls.add(item['link'])
            unique_url_data.append(item)
    final_data = []
    for item in unique_url_data:
        is_duplicate = False
        for existing_item in final_data:
            if similarity(item['title'], existing_item['title']) > 0.85:
                is_duplicate = True
                break
        if not is_duplicate:
            final_data.append(item)
    return final_data

# --- 5. ä¸»åŸ·è¡Œæµç¨‹ ---
def run_all():
    all_data = []
    SITES = ['racing_post', 'scmp_racing', 'singtao_racing', 'punters_au', 'racing_com', 'tospo_keiba', 'netkeiba_news', 'bloodhorse_news', 'the_straight', 'anz_bloodstock', 'ttr_ausnz', 'smh_racing', 'drf_news', 'racenet_news', 'daily_telegraph', 'equidia_racing']
    
    for site in SITES:
        try:
            print(f"\n>>> ä»»å‹™é–‹å§‹: {site}")
            module = importlib.import_module(f"scrapers.{site}")
            data = module.scrape()
            if data:
                for item in data: item['source'] = site
                all_data.extend(data)
                print(f"    âœ… æŠ“åˆ° {len(data)} å‰‡")
        except Exception as e:
            print(f"    âŒ {site} éŒ¯èª¤: {e}")

    if all_data:
        all_data = deduplicate_data(all_data)
        all_data = translate_titles_to_en(all_data)

        date_str = datetime.now().strftime('%Y%m%d')
        os.makedirs('data', exist_ok=True)

        df = pd.DataFrame(all_data)
        df.to_csv(f"data/raw_news_{date_str}.csv", index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ CSV å·²å­˜è‡³: data/raw_news_{date_str}.csv")

        # å ±å‘Šç”Ÿæˆå‰çš„æœ€çµ‚å†·å»
        print(f"\nâŒ› æ­£åœ¨ç‚º AI åˆ†æé€²è¡Œæœ€å¾Œå†·å» (10ç§’)...")
        time.sleep(10)

        print(f"\nğŸ¤– å•Ÿå‹• AI æˆ°ç•¥åˆ†ææ¨¡å¼...")
        strategic_brief = generate_strategic_brief(all_data)
        
        md_filename = f"data/strategic_report_{date_str}.md"
        with open(md_filename, "w", encoding="utf-8") as f:
            f.write(strategic_brief)
        print(f"âœ¨ æˆ°ç•¥å ±å‘Šå·²ç”Ÿæˆ: {md_filename}")
    else:
        print("\nâŒ ä»Šæ—¥ç„¡æ•¸æ“šã€‚")

if __name__ == "__main__":
    run_all()
