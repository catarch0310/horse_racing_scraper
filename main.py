import pandas as pd
from datetime import datetime
import os
import importlib
import google.generativeai as genai
import time
from difflib import SequenceMatcher

# --- 1. AI æ ¸å¿ƒè¨­å®šèˆ‡è‡ªå‹•åµæ¸¬ ---
API_KEY = os.getenv("GEMINI_API_KEY")

def get_best_model():
    if not API_KEY: return None
    genai.configure(api_key=API_KEY)
    
    # å˜—è©¦å¤šå€‹æ¨¡å‹åç¨±æ ¼å¼
    candidate_names = ['gemini-1.5-flash', 'models/gemini-1.5-flash', 'gemini-1.5-pro']
    
    print("ğŸ¤– æ­£åœ¨åµæ¸¬å¯ç”¨ AI æ¨¡å‹...")
    for name in candidate_names:
        try:
            model = genai.GenerativeModel(name)
            # æ¸¬è©¦è«‹æ±‚
            model.generate_content("hi", generation_config={"max_output_tokens": 1})
            print(f"âœ… æˆåŠŸå•Ÿç”¨æ¨¡å‹: {name}")
            return model
        except:
            continue
    return None

model_instance = get_best_model()

# --- 2. è³‡æ–™æ¸…æ´—å·¥å…· ---
def similarity(a, b):
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def deduplicate_data(all_data):
    if not all_data: return []
    print(f"\nğŸ§¹ é–‹å§‹è³‡æ–™æ¸…æ´— (åŸå§‹ç¸½æ•¸: {len(all_data)} å‰‡)...")
    
    unique_url_data = []
    seen_urls = set()
    for item in all_data:
        if item['link'] not in seen_urls:
            seen_urls.add(item['link'])
            unique_url_data.append(item)
    
    final_data = []
    for item in unique_url_data:
        is_duplicate = False
        # æé«˜é–€æª»åˆ° 0.85ï¼Œæ¸›å°‘èª¤åˆª
        for existing_item in final_data:
            if similarity(item['title'], existing_item['title']) > 0.85:
                is_duplicate = True
                break
        if not is_duplicate:
            final_data.append(item)
            
    print(f"âœ¨ æ¸…æ´—å®Œæˆ: æœ€çµ‚ä¿ç•™ {len(final_data)} å‰‡")
    return final_data

# --- 3. AI å ±å‘Šç”Ÿæˆ (æ ¸å¿ƒä¿®æ­£) ---
def generate_ai_report(cleaned_headlines):
    if not model_instance:
        return "AI å ±å‘Šç”Ÿæˆå¤±æ•—ï¼šæ¨¡å‹æœªå°±ç·’ã€‚"

    news_list_text = ""
    for i, item in enumerate(cleaned_headlines):
        news_list_text += f"{i+1}. [{item['source']}] {item['title']}\n"

    print(f"   [AI] æ­£åœ¨è™•ç†æ–‡æœ¬é•·åº¦: {len(news_list_text)} å­—å…ƒ")

    prompt = f"""
    You are a Global Horse Racing Chief Editor. Analyze these headlines:
    {news_list_text}
    
    Please generate a report in THREE parts in this exact order:
    1. ENGLISH VERSION
    2. TRADITIONAL CHINESE VERSION (HONG KONG)
    3. JAPANESE VERSION

    For each language:
    - **Top 5 Priority**: 5 most important global stories + one-sentence significance.
    - **Categorized Highlights**: Group others into "HK Racing", "International Racing", and "Analysis".
    - **Global Trend**: 100-word analysis of today's atmosphere.

    --- MANDATORY INSTRUCTIONS ---
    - CHINESE: Use Traditional Chinese (Hong Kong). Horse/Person/Race names MUST match official Hong Kong Jockey Club (HKJC) translations. 
    - JAPANESE: Use professional terminology (e.g., è¿½ã„åˆ‡ã‚Š, é‡è³).
    - FORMAT: Use Markdown with bold headers.
    """

    try:
        # é—œéµä¿®æ­£ï¼šé—œé–‰å…§å®¹å®‰å…¨éæ¿¾ (Safety Settings)
        # è³½é¦¬å…§å®¹å¸¸è¢« AI èª¤èªç‚ºéæ³•è³­åšè€Œå°é–ï¼Œé€™è£¡å¼·è¡Œé–‹å•Ÿ
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]

        response = model_instance.generate_content(
            prompt, 
            generation_config={"max_output_tokens": 4000, "temperature": 0.7},
            safety_settings=safety_settings
        )
        
        if response.text:
            return response.text.strip()
        else:
            return "AI å›å‚³å…§å®¹ç‚ºç©ºï¼Œå¯èƒ½è¢«å…§å®¹éæ¿¾å™¨é˜»æ“‹ã€‚"
            
    except Exception as e:
        return f"AI å ±å‘Šå…§å®¹ç”Ÿæˆå‡ºéŒ¯: {str(e)}"

# --- 4. åŸ·è¡Œæµç¨‹ ---
def run_all():
    raw_collected_data = []
    # ç¢ºä¿é€™ 11 å€‹åç¨±èˆ‡ scrapers è³‡æ–™å¤¾æª”æ¡ˆä¸€è‡´
    SITES = [
        'racing_post', 'scmp_racing', 'singtao_racing', 'punters_au', 
        'racing_com', 'tospo_keiba', 'netkeiba_news', 'bloodhorse_news', 
        'the_straight', 'anz_bloodstock', 'ttr_ausnz'
    ]
    
    for site in SITES:
        try:
            print(f"\n>>> ä»»å‹™é–‹å§‹: {site}")
            module = importlib.import_module(f"scrapers.{site}")
            data = module.scrape()
            if data:
                for item in data: item['source'] = site
                raw_collected_data.extend(data)
                print(f"    âœ… æŠ“åˆ° {len(data)} å‰‡")
        except Exception as e:
            print(f"    âŒ {site} éŒ¯èª¤: {e}")

    # 1. æ¸…æ´—æ•¸æ“š
    cleaned_data = deduplicate_data(raw_collected_data)

    if cleaned_data:
        date_str = datetime.now().strftime('%Y%m%d')
        os.makedirs('data', exist_ok=True)

        # 2. å„²å­˜ CSV
        df = pd.DataFrame(cleaned_data)
        df.to_csv(f"data/raw_news_{date_str}.csv", index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ CSV å·²å­˜è‡³: data/raw_news_{date_str}.csv")

        # 3. ç”Ÿæˆ AI å ±å‘Š
        print(f"\nğŸ¤– å•Ÿå‹• AI ç¸½ç·¨è¼¯æ¨¡å¼ (ä¸‰èªè¼¸å‡º)...")
        ai_report_content = generate_ai_report(cleaned_data)
        
        md_filename = f"data/racing_report_{date_str}.md"
        with open(md_filename, "w", encoding="utf-8") as f:
            f.write(ai_report_content)
        
        print(f"âœ¨ AI æˆ°å ±å·²ç”Ÿæˆ: {md_filename}")
        # åœ¨çµ‚ç«¯æ©Ÿå°å‡ºå‰ 100 å­—ç¢ºèªæœ‰å…§å®¹
        print(f"--- å…§å®¹é è¦½ ---\n{ai_report_content[:100]}...")
    else:
        print("\nâŒ ä»Šæ—¥ç„¡æ•¸æ“šï¼Œä¸ç”Ÿæˆå ±å‘Šã€‚")

if __name__ == "__main__":
    run_all()
