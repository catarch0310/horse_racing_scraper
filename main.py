import pandas as pd
from datetime import datetime
import os
import importlib
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
import time

# --- AI è¨­å®š ---
API_KEY = os.getenv("GEMINI_API_KEY")

def get_ai_response(title, content):
    if not API_KEY:
        return "AI å¤±æ•—: æ‰¾ä¸åˆ° API KEY"
    
    try:
        genai.configure(api_key=API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        text = content[:2000] if len(content) > 50 else f"æ ¹æ“šæ¨™é¡Œåˆ†æï¼š{title}"
        prompt = f"è«‹ç”¨ç¹é«”ä¸­æ–‡å°‡ä»¥ä¸‹è³½é¦¬æ–°èç¸®å¯«æˆä¸€å¥ç´„ 50 å­—çš„æ‘˜è¦ï¼š\n\n{text}"
        
        # åŠ ä¸Šå®‰å…¨è¨­å®šï¼Œé˜²æ­¢ AI æ‹’çµ•ç”Ÿæˆ
        response = model.generate_content(prompt, safety_settings=[
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ])
        return response.text.strip()
    except Exception as e:
        if "429" in str(e):
            return "AI å¤±æ•—: è«‹æ±‚éå¿« (429)"
        return f"AI å¤±æ•—: {str(e)[:50]}"

def get_full_text(url, source):
    if source == 'on_cc_racing': return ""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.find_all('p')
        return "\n".join([p.get_text() for p in paragraphs if len(p.get_text()) > 30])[:2500]
    except:
        return ""

def run_all():
    all_data = []
    SITES = ['racing_post', 'scmp_racing', 'on_cc_racing', 'punters_au']
    
    for site in SITES:
        try:
            print(f"\n>>> ä»»å‹™: {site}")
            module = importlib.import_module(f"scrapers.{site}")
            data = module.scrape()
            if data:
                for item in data: item['source'] = site
                all_data.extend(data)
                print(f"    æˆåŠŸæŠ“åˆ° {len(data)} å‰‡")
        except Exception as e:
            print(f"    âŒ åŸ·è¡ŒéŒ¯èª¤: {e}")

    if all_data:
        print(f"\nğŸ¤– å•Ÿå‹• AI æ‘˜è¦ (å…± {len(all_data)} å‰‡)...")
        for i, item in enumerate(all_data):
            print(f"    ({i+1}/{len(all_data)}) è™•ç†: {item['title'][:20]}")
            # ç²å–å…§æ–‡
            full_text = get_full_text(item['link'], item['source'])
            # ç”¢ç”Ÿæ‘˜è¦
            item['ai_summary'] = get_ai_response(item['title'], full_text)
            # å»¶é² 5 ç§’ä»¥ç¬¦åˆå…è²»ç‰ˆé™åˆ¶
            time.sleep(5)

        df = pd.DataFrame(all_data)
        os.makedirs('data', exist_ok=True)
        filename = f"data/racing_report_{datetime.now().strftime('%Y%m%d')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ä»»å‹™å®Œæˆï¼æª”æ¡ˆå­˜è‡³: {filename}")

if __name__ == "__main__":
    run_all()
